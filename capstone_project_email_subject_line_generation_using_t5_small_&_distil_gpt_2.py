# -*- coding: utf-8 -*-
"""CAPSTONE PROJECT: EMAIL SUBJECT LINE GENERATION USING T5 SMALL & DISTIL GPT 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zphflwUd-erz1384ztpFMPUD7L8hjPYx
"""

!pip install transformers datasets evaluate

# Import Libraries
import os
import random
import torch
import evaluate
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.optim import AdamW
from tqdm import tqdm
import torch.nn as nn
from wordcloud import WordCloud
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from nltk.corpus import stopwords

# Clone the repository
!git clone https://github.com/ryanzhumich/AESLC.git

def load_data_to_dataframe(folder_path):
    data = []
    for file_name in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file_name)
        print(f"Processing file: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.read().strip().split('\n')

            email_body = []
            subject_line = None
            in_subject_section = False  # Track if we are in the `@subject` section

            for line in lines:
                # Detect subject line
                if line.startswith('@subject'):
                    in_subject_section = True
                    subject_line = line.replace('@subject', '').strip()
                elif in_subject_section and not line.startswith('@'):  # If it's part of the subject section
                    subject_line += ' ' + line.strip()
                elif line.startswith('@'):  # Stop collecting subject when other annotations start
                    in_subject_section = False
                elif not line.startswith('@'):  # Collect email body
                    email_body.append(line)

            # Debug the parsed components
            print(f"Parsed email_body: {email_body}")
            print(f"Parsed subject_line: {subject_line}")

            # Only add valid entries
            if email_body and subject_line:
                data.append({
                    'email': ' '.join(email_body).strip(),
                    'subject': subject_line.strip()
                })
            else:
                print(f"Skipped file due to missing data: {file_name}")

    print(f"Total records loaded: {len(data)}")
    return pd.DataFrame(data)

# Load datasets
train_data = load_data_to_dataframe('AESLC/enron_subject_line/train')
dev_data = load_data_to_dataframe('AESLC/enron_subject_line/dev')
test_data = load_data_to_dataframe('AESLC/enron_subject_line/test')

# Display the first 3 rows of train dataset
train_data[:3]

# Display the first 5 rows of train dataset
train_data.head()

# Display the first 5 rows of test dataset
test_data.head()

# Display the first 5 rows of dev dataset
dev_data.head()

# Ensure the splits have the expected sizes
print(f"Train: {len(train_data)}, Dev: {len(dev_data)}, Test: {len(test_data)}")

# Combine all datasets without adding the split column
all_data = pd.concat([train_data, dev_data, test_data], ignore_index=True)

# Inspect the combined dataset
all_data.head()

# General information about the dataset
print(all_data.info())

# Check for missing values in all columns
print(all_data.isnull().sum())

import nltk
nltk.download('stopwords')

# Clean Text
def clean_text(text):
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove special characters but retain letters, numbers, and spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Normalize spaces (remove extra spaces)
    text = ' '.join(text.split())

    # Remove stopwords
    # stop_words = set(stopwords.words("english"))
    # text = ' '.join(word for word in text.split() if word not in stop_words)

    return text

# Apply the updated clean_text function
all_data['email'] = all_data['email'].apply(clean_text)
all_data['subject'] = all_data['subject'].apply(clean_text)

# Check the cleaned data
all_data.head()

# Calculate word counts
all_data['email_word_count'] = all_data['email'].apply(lambda x: len(x.split()))
all_data['subject_word_count'] = all_data['subject'].apply(lambda x: len(x.split()))

# Plot histograms
plt.figure(figsize=(12, 6))

# Email word count distribution
plt.subplot(1, 2, 1)
plt.hist(all_data['email_word_count'], bins=30, color='blue', alpha=0.7)
plt.title('Email Word Count Distribution')
plt.xlabel('Word Count')
plt.ylabel('Frequency')

# Subject line word count distribution
plt.subplot(1, 2, 2)
plt.hist(all_data['subject_word_count'], bins=30, color='orange', alpha=0.7)
plt.title('Subject Line Word Count Distribution')
plt.xlabel('Word Count')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Correlation between email and subject line lengths
correlation = np.corrcoef(all_data['email_word_count'], all_data['subject_word_count'])[0, 1]
print(f"Correlation between email and subject line word counts: {correlation}")

# Find and remove duplicates
print(f"Number of duplicate rows: {all_data.duplicated().sum()}")
all_data = all_data.drop_duplicates()

# Check for duplicates in all_data after dropping them
print(f"Number of duplicate rows: {all_data.duplicated().sum()}")

# Email body word cloud
email_words = " ".join(all_data['email']).split()
email_wordcloud = WordCloud(width=800, height=400).generate(" ".join(email_words))
plt.figure(figsize=(10, 5))
plt.imshow(email_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Email Body Word Cloud')
plt.show()

# Subject line word cloud
subject_words = " ".join(all_data['subject']).split()
subject_wordcloud = WordCloud(width=800, height=400).generate(" ".join(subject_words))
plt.figure(figsize=(10, 5))
plt.imshow(subject_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Subject Line Word Cloud')
plt.show()

# Identify outliers in email lengths or subject line lengths.
# Boxplot for email word counts
plt.boxplot(all_data['email_word_count'], vert=False, patch_artist=True)
plt.title('Email Word Count Boxplot')
plt.xlabel('Word Count')
plt.show()

# Boxplot for subject line word counts
plt.boxplot(all_data['subject_word_count'], vert=False, patch_artist=True)
plt.title('Subject Line Word Count Boxplot')
plt.xlabel('Word Count')
plt.show()

# Identify outliers using InterQuartile Range(IQR)
def identify_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]

# Identify outliers
email_outliers = identify_outliers(all_data, 'email_word_count')
subject_outliers = identify_outliers(all_data, 'subject_word_count')

print(f"Email outliers: {len(email_outliers)}")
print(f"Subject line outliers: {len(subject_outliers)}")

# Remove outliers and clean the dataset
def remove_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

# Cleaned dataset
cleaned_data = remove_outliers(all_data, 'email_word_count')
cleaned_data = remove_outliers(cleaned_data, 'subject_word_count')

print(f"Cleaned data size: {len(cleaned_data)}")

"""# T5 - SMALL"""

# Prepare the data for model training
def prepare_data_for_training(data):
    """
    Prepares the dataset for training by formatting it into input-output pairs.
    Args:
        data (pd.DataFrame): Cleaned and preprocessed dataset.
    Returns:
        pd.DataFrame: Dataset with 'input_text' and 'target_text' columns.
    """
    # Ensure columns exist in the input data
    if 'email' not in data.columns or 'subject' not in data.columns:
        raise ValueError("Input data must contain 'email' and 'subject' columns.")

    # Combine email body with a prefix to make input text
    data['input_text'] = "summarize: " + data['email']
    data['target_text'] = data['subject']

    # Select relevant columns
    prepared_data = data[['input_text', 'target_text']]
    return prepared_data

# Split data into training, validation, and test sets
def split_data(data, train_size=0.8, random_state=42):
    """
    Splits the dataset into training, validation, and test sets.
    Args:
        data (pd.DataFrame): Dataset with 'input_text' and 'target_text' columns.
        train_size (float): Proportion of the dataset to include in the training split.
        random_state (int): Random state for reproducibility.
    Returns:
        dict: A dictionary containing 'train', 'validation', and 'test' DataFrames.
    """
    # Ensure the dataset is not empty
    if data.empty:
        raise ValueError("Input data is empty. Ensure the dataset is properly loaded and cleaned.")

    # Split into training and temp datasets
    train_data, temp_data = train_test_split(data, train_size=train_size, random_state=random_state)

    # Further split temp_data into validation and test datasets
    validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=random_state)

    return {
        'train': train_data,
        'validation': validation_data,
        'test': test_data,
    }

# Display dataset statistics
def display_dataset_stats(splits):
    """
    Displays basic statistics for the training, validation, and test datasets.
    Args:
        splits (dict): Dictionary containing 'train', 'validation', and 'test' DataFrames.
    """
    for split_name, dataset in splits.items():
        print(f"{split_name.capitalize()} Dataset:")
        print(f"  - Total Records: {len(dataset)}")
        print(f"  - Avg Input Length: {dataset['input_text'].apply(len).mean():.2f}")
        print(f"  - Avg Target Length: {dataset['target_text'].apply(len).mean():.2f}")
        print()

# Workflow
try:
    # Prepare data
    prepared_data = prepare_data_for_training(all_data)

    # Split data into train, validation, and test sets
    splits = split_data(prepared_data)

    # Display dataset statistics
    display_dataset_stats(splits)

    # Save splits for later use
    splits['train'].to_csv('train_data.csv', index=False)
    splits['validation'].to_csv('validation_data.csv', index=False)
    splits['test'].to_csv('test_data.csv', index=False)

    print("Data preprocessing and preparation completed successfully.")
except Exception as e:
    print(f"Error during data preparation: {e}")

# Check for GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class EmailDataset(Dataset):
    """
    Custom dataset class for email summarization.
    """
    def __init__(self, data, tokenizer, max_input_length=512, max_target_length=64):
        """
        Initialize the dataset.

        Args:
            data (pd.DataFrame): Dataset containing 'input_text' and 'target_text'.
            tokenizer: Pretrained tokenizer from HuggingFace.
            max_input_length (int): Maximum length for input sequences.
            max_target_length (int): Maximum length for target sequences.
        """
        self.data = data
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        input_text = self.data.iloc[idx]['input_text']
        target_text = self.data.iloc[idx]['target_text']

        # Tokenize input and target text
        inputs = self.tokenizer(
            input_text,
            max_length=self.max_input_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
        )
        targets = self.tokenizer(
            target_text,
            max_length=self.max_target_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
        )

        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': targets['input_ids'].squeeze(),
        }

# Load tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-small')

# Initialize datasets
train_dataset = EmailDataset(splits['train'], tokenizer)
validation_dataset = EmailDataset(splits['validation'], tokenizer)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
validation_loader = DataLoader(validation_dataset, batch_size=8)

# Load model
model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)

# Print example tokenized data
sample_batch = next(iter(train_loader))
print("Input IDs:", sample_batch['input_ids'][0])
print("Attention Mask:", sample_batch['attention_mask'][0])
print("Labels:", sample_batch['labels'][0])

# Define training function
def train_model(model, train_loader, validation_loader, epochs, optimizer, device):
    """
    Train the T5 model.

    Args:
        model: The T5 model to train.
        train_loader: DataLoader for the training dataset.
        validation_loader: DataLoader for the validation dataset.
        epochs (int): Number of epochs.
        optimizer: Optimizer for training.
        device: Device to use for training ('cpu' or 'cuda').

    Returns:
        None
    """
    model.train()
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        total_loss = 0

        # Training loop
        for batch in tqdm(train_loader):
            optimizer.zero_grad()

            # Move data to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
            )
            loss = outputs.loss

            # Backward pass
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Training Loss: {avg_loss:.4f}")

        # Validate model
        validate_model(model, validation_loader, device)

# Define validation function
def validate_model(model, validation_loader, device):
    """
    Validate the T5 model.

    Args:
        model: The T5 model to validate.
        validation_loader: DataLoader for the validation dataset.
        device: Device to use for validation ('cpu' or 'cuda').

    Returns:
        None
    """
    model.eval()
    total_loss = 0
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    with torch.no_grad():
        for batch in tqdm(validation_loader):
            # Move data to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
            )
            loss = outputs.loss
            total_loss += loss.item()

    avg_loss = total_loss / len(validation_loader)
    print(f"Validation Loss: {avg_loss:.4f}")

# Define optimizer
optimizer = AdamW(model.parameters(), lr=1e-3)

# Train the model
train_model(
    model=model,
    train_loader=train_loader,
    validation_loader=validation_loader,
    epochs=3,
    optimizer=optimizer,
    device=device,
)

# Save the model
model.save_pretrained('email_subject_line_generator_model')
tokenizer.save_pretrained('email_subject_line_generator_model')
print("Model and tokenizer saved successfully.")

# Evaluation: Generate predictions
def generate_predictions(model, tokenizer, test_loader, device):
    """
    Generate predictions on the test set.

    Args:
        model: Trained T5 model.
        tokenizer: Tokenizer for the model.
        test_loader: DataLoader for the test dataset.
        device: Device to use for evaluation ('cpu' or 'cuda').

    Returns:
        List of predictions.
    """
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch in tqdm(test_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=64,
                num_beams=4,
                early_stopping=True,
            )
            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            predictions.extend(decoded_preds)
            actuals.extend(decoded_labels)

    return predictions, actuals

# Evaluate model on the test set
test_dataset = EmailDataset(splits['test'], tokenizer)
test_loader = DataLoader(test_dataset, batch_size=8)
predictions, actuals = generate_predictions(model, tokenizer, test_loader, device)

# Display sample predictions
for i in range(5):
    print(f"Input: {splits['test'].iloc[i]['input_text']}")
    print(f"Actual: {actuals[i]}")
    print(f"Predicted: {predictions[i]}")
    print("------")

!pip install rouge_score

import torch
from tqdm.auto import tqdm
from evaluate import load

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Model Evaluation with prediction loop
metric = load("rouge")

# Prediction loop with smaller batches
predictions = []
# Iterate directly over the test split of the original data
for batch_start in tqdm(range(0, len(splits['test']), 4), total=len(splits['test']) // 4):
    batch_end = min(batch_start + 4, len(splits['test']))  # Handle the last batch
    batch_data = splits['test'][batch_start:batch_end]  # Get a slice of the data

    # Prepare batch for model input
    batch = {
        'input_ids': torch.tensor(tokenizer(batch_data['input_text'].tolist(), padding=True, truncation=True, return_tensors='pt')['input_ids']).to(device),
        'attention_mask': torch.tensor(tokenizer(batch_data['input_text'].tolist(), padding=True, truncation=True, return_tensors='pt')['attention_mask']).to(device)
    }

    with torch.no_grad():
        outputs = model.generate(**batch)

    predictions.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))
    torch.cuda.empty_cache()  # Clear cache after each batch

# Calculate ROUGE scores
references = splits['test']['target_text'].tolist()  # Get references from the split
results = metric.compute(predictions=predictions, references=references)
print(results)

# Load the fine-tuned model and tokenizer
model_path = "email_subject_line_generator_model"
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def generate_subject_line(email_body):
    """
    Generates a subject line for the given email body using the fine-tuned T5 model.

    Args:
        email_body (str): The body of the email.

    Returns:
        str: The generated subject line.
    """
    input_text = "summarize: " + email_body
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(device)

    with torch.no_grad():
        outputs = model.generate(input_ids)

    generated_subject = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_subject

# Example usage:
email_body = '''
Just a quick reminder about our marketing strategy planning meeting scheduled for tomorrow, April 27th, at 2:00 PM IST in Conference Room A.
Please come prepared to discuss your team's Q3 initiatives and bring any relevant data or reports.
'''
generated_subject = generate_subject_line(email_body)
print("Generated Subject Line:", generated_subject)

"""# DISTILL GPT 2"""

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# --- 1. Data Preparation ---

def prepare_data_for_training(data):
    """
    Prepares the dataset for training DistilGPT-2.
    For GPT models, input is combined with the target as a single sequence.
    """
    if 'email' not in data.columns or 'subject' not in data.columns:
        raise ValueError("Input data must contain 'email' and 'subject' columns.")

    # Combine email body and subject with a special separator (e.g., "<|endoftext|>")
    data['combined_text'] = data['email'] + " \nSubject: " + data['subject'] + " <|endoftext|>"

    return data[['combined_text']]

def split_data(data, train_size=0.8, random_state=42):
    """
    Split data into train, validation, and test sets.
    """
    if data.empty:
        raise ValueError("Input data is empty.")

    train_data, temp_data = train_test_split(data, train_size=train_size, random_state=random_state)
    validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=random_state)

    return {
        'train': train_data,
        'validation': validation_data,
        'test': test_data,
    }

def display_dataset_stats(splits):
    """
    Displays dataset statistics.
    """
    for split_name, dataset in splits.items():
        print(f"{split_name.capitalize()} Dataset:")
        print(f"  - Total Records: {len(dataset)}")
        print(f"  - Avg Combined Text Length: {dataset['combined_text'].apply(len).mean():.2f}")
        print()

# --- 2. Dataset Class ---

class EmailDataset(Dataset):
    """
    Custom dataset for fine-tuning DistilGPT-2.
    """
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx]['combined_text']

        encodings = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors="pt"
        )

        input_ids = encodings['input_ids'].squeeze()
        attention_mask = encodings['attention_mask'].squeeze()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': input_ids  # For GPT models, labels = inputs
        }

# --- 3. Load Tokenizer and Model ---
tokenizer = AutoTokenizer.from_pretrained('distilgpt2')
tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no pad token, so reuse EOS token for padding

model = AutoModelForCausalLM.from_pretrained('distilgpt2')
model.resize_token_embeddings(len(tokenizer))
model = model.to(device)

# --- 4. Full Data Flow ---

# Assuming your data is in `all_data`
try:
    prepared_data = prepare_data_for_training(all_data)
    splits = split_data(prepared_data)
    display_dataset_stats(splits)

    splits['train'].to_csv('train_data.csv', index=False)
    splits['validation'].to_csv('validation_data.csv', index=False)
    splits['test'].to_csv('test_data.csv', index=False)
    print("Data preprocessing completed.")
except Exception as e:
    print(f"Error: {e}")

# Dataloaders
train_dataset = EmailDataset(splits['train'], tokenizer)
validation_dataset = EmailDataset(splits['validation'], tokenizer)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
validation_loader = DataLoader(validation_dataset, batch_size=8)

# --- 5. Training Functions ---

def train_model(model, train_loader, validation_loader, optimizer, epochs, device):
    model.train()
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        total_loss = 0

        for batch in tqdm(train_loader):
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        print(f"Training Loss: {avg_train_loss:.4f}")

        validate_model(model, validation_loader, device)

def validate_model(model, validation_loader, device):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(validation_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss

            total_loss += loss.item()

    avg_val_loss = total_loss / len(validation_loader)
    print(f"Validation Loss: {avg_val_loss:.4f}")
    model.train()

# Optimizer
optimizer = AdamW(model.parameters(), lr=1e-3)

# Train
train_model(model, train_loader, validation_loader, optimizer, epochs=2, device=device)

# Save model and tokenizer
model.save_pretrained('email_subject_generator_distilgpt2')
tokenizer.save_pretrained('email_subject_generator_distilgpt2')
print("Model saved successfully.")

# --- 6. Evaluation and Generation ---

# Test set
test_dataset = EmailDataset(splits['test'], tokenizer)
test_loader = DataLoader(test_dataset, batch_size=4)

def generate_predictions(model, tokenizer, test_loader, device):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch in tqdm(test_loader):
            input_ids = batch['input_ids'].to(device)

            generated_ids = model.generate(
                input_ids=input_ids,
                attention_mask=batch['attention_mask'].to(device),
                max_new_tokens=64,
                num_return_sequences=1,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=0.7
            )

            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            targets = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)

            predictions.extend(preds)
            actuals.extend(targets)

    return predictions, actuals

# Generate
predictions, actuals = generate_predictions(model, tokenizer, test_loader, device)

# Print few predictions
for i in range(5):
    print(f"Input: {splits['test'].iloc[i]['combined_text']}")
    print(f"Actual: {actuals[i]}")
    print(f"Predicted: {predictions[i]}")
    print("------")

# Evaluate using ROUGE
# metric = load("rouge")
# results = metric.compute(predictions=predictions, references=actuals)
# print(results)

!pip install rouge_score

# Evaluate using ROUGE
from evaluate import load
metric = load("rouge")
results = metric.compute(predictions=predictions, references=actuals)
print(results)

# Load the fine-tuned model and tokenizer
model_path = "email_subject_generator_distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path) # Corrected this line

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def generate_subject_line(email_body):
    """
    Generates a subject line for the given email body using the fine-tuned T5 model.

    Args:
        email_body (str): The body of the email.

    Returns:
        str: The generated subject line.
    """
    input_text = "summarize: " + email_body
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(device)

    with torch.no_grad():
        outputs = model.generate(input_ids)

    generated_subject = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_subject

# Example usage:
email_body = '''
Just a quick reminder about our marketing strategy planning meeting scheduled for tomorrow, April 27th, at 2:00 PM IST in Conference Room A.
Please come prepared to discuss your team's Q3 initiatives and bring any relevant data or reports.
'''
generated_subject = generate_subject_line(email_body)
print("Generated Subject Line:", generated_subject)